{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VISHAL4411835/DataScienceProject/blob/main/convolutional_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLcQaW5kMqka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab7dc3f-33ef-4479-bbfe-ea79eebebf43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRX0kOZJNfPt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFuoLkJfN8ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6405ff3-6fb0-4ef9-910c-c484a7e95eca"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvE-heJNo3GG"
      },
      "source": [
        "### Preprocessing the Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q1byuXGcVC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21dfa34-a8d0-468b-f2e3-44cc6bf50562"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "training_set = train_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/Dataset/dataset/training_set',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM6p75nOeLrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06670f26-5de1-4097-d952-04e29f2f919a"
      },
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_set = test_datagen.flow_from_directory(\n",
        "        '/content/drive/MyDrive/Colab Notebooks/Dataset/dataset/test_set',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1963 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfy9-Ye9f7MH"
      },
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5YJj_XMl5LF"
      },
      "source": [
        "### Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiY3KgWugeeY"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu',input_shape = [64,64,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0h-O9vVhkW5"
      },
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(3,3),strides = 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPswYbVxjSOH"
      },
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(32,kernel_size=3,activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Flattening"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd5I1SN0jZJg"
      },
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxeMDR4vjxbX"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx8_GHgdkTcx"
      },
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJCOaezlE_6"
      },
      "source": [
        "cnn.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxuNec_8lsiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7e8cf6-830b-4f38-d9dc-43b1144441d5"
      },
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "250/250 [==============================] - 2464s 10s/step - loss: 0.8038 - accuracy: 0.5260 - val_loss: 0.7071 - val_accuracy: 0.5945\n",
            "Epoch 2/25\n",
            "250/250 [==============================] - 91s 365ms/step - loss: 0.6483 - accuracy: 0.6276 - val_loss: 0.5777 - val_accuracy: 0.7025\n",
            "Epoch 3/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.5920 - accuracy: 0.6815 - val_loss: 0.5684 - val_accuracy: 0.7056\n",
            "Epoch 4/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.5633 - accuracy: 0.7043 - val_loss: 0.5382 - val_accuracy: 0.7361\n",
            "Epoch 5/25\n",
            "250/250 [==============================] - 91s 365ms/step - loss: 0.5264 - accuracy: 0.7355 - val_loss: 0.4955 - val_accuracy: 0.7585\n",
            "Epoch 6/25\n",
            "250/250 [==============================] - 91s 362ms/step - loss: 0.4982 - accuracy: 0.7576 - val_loss: 0.4839 - val_accuracy: 0.7738\n",
            "Epoch 7/25\n",
            "250/250 [==============================] - 91s 364ms/step - loss: 0.5031 - accuracy: 0.7517 - val_loss: 0.5344 - val_accuracy: 0.7494\n",
            "Epoch 8/25\n",
            "250/250 [==============================] - 93s 370ms/step - loss: 0.4745 - accuracy: 0.7732 - val_loss: 0.4682 - val_accuracy: 0.7799\n",
            "Epoch 9/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.4661 - accuracy: 0.7782 - val_loss: 0.4722 - val_accuracy: 0.7809\n",
            "Epoch 10/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.4450 - accuracy: 0.7929 - val_loss: 0.4840 - val_accuracy: 0.7616\n",
            "Epoch 11/25\n",
            "250/250 [==============================] - 90s 361ms/step - loss: 0.4244 - accuracy: 0.8023 - val_loss: 0.4552 - val_accuracy: 0.7840\n",
            "Epoch 12/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.4199 - accuracy: 0.8076 - val_loss: 0.4857 - val_accuracy: 0.7860\n",
            "Epoch 13/25\n",
            "250/250 [==============================] - 90s 360ms/step - loss: 0.3994 - accuracy: 0.8210 - val_loss: 0.5279 - val_accuracy: 0.7621\n",
            "Epoch 14/25\n",
            "250/250 [==============================] - 90s 361ms/step - loss: 0.3825 - accuracy: 0.8246 - val_loss: 0.4781 - val_accuracy: 0.7825\n",
            "Epoch 15/25\n",
            "250/250 [==============================] - 92s 368ms/step - loss: 0.3769 - accuracy: 0.8310 - val_loss: 0.4886 - val_accuracy: 0.7901\n",
            "Epoch 16/25\n",
            "250/250 [==============================] - 90s 360ms/step - loss: 0.3742 - accuracy: 0.8269 - val_loss: 0.4613 - val_accuracy: 0.7942\n",
            "Epoch 17/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.3502 - accuracy: 0.8470 - val_loss: 0.4663 - val_accuracy: 0.7855\n",
            "Epoch 18/25\n",
            "250/250 [==============================] - 91s 362ms/step - loss: 0.3353 - accuracy: 0.8448 - val_loss: 0.4848 - val_accuracy: 0.7871\n",
            "Epoch 19/25\n",
            "250/250 [==============================] - 91s 362ms/step - loss: 0.3248 - accuracy: 0.8575 - val_loss: 0.4578 - val_accuracy: 0.8029\n",
            "Epoch 20/25\n",
            "250/250 [==============================] - 90s 358ms/step - loss: 0.3157 - accuracy: 0.8600 - val_loss: 0.4825 - val_accuracy: 0.8008\n",
            "Epoch 21/25\n",
            "250/250 [==============================] - 91s 363ms/step - loss: 0.3063 - accuracy: 0.8660 - val_loss: 0.5013 - val_accuracy: 0.8023\n",
            "Epoch 22/25\n",
            "250/250 [==============================] - 92s 366ms/step - loss: 0.2836 - accuracy: 0.8771 - val_loss: 0.5168 - val_accuracy: 0.7906\n",
            "Epoch 23/25\n",
            "250/250 [==============================] - 91s 362ms/step - loss: 0.2689 - accuracy: 0.8848 - val_loss: 0.4670 - val_accuracy: 0.8110\n",
            "Epoch 24/25\n",
            "250/250 [==============================] - 90s 360ms/step - loss: 0.2489 - accuracy: 0.8969 - val_loss: 0.6024 - val_accuracy: 0.7697\n",
            "Epoch 25/25\n",
            "250/250 [==============================] - 89s 358ms/step - loss: 0.2666 - accuracy: 0.8858 - val_loss: 0.5539 - val_accuracy: 0.7866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f893bc2e150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Making a single prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCrGJFY7m7cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d61c4e2-b4de-48d9-dbf5-1ae4aa1f4026"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "test_img = image.load_img('/content/drive/MyDrive/Colab Notebooks/Dataset/dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
        "test_img = image.img_to_array(test_img)\n",
        "test_img = np.expand_dims(test_img,axis = 0)\n",
        "result = cnn.predict(test_img)\n",
        "training_set.class_indices\n",
        "print(training_set.class_indices)\n",
        "if result[0][0] == 1:\n",
        "  prediction = 'Dog'\n",
        "else:\n",
        "  predictiom = 'Cat'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'cats': 0, 'dogs': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZO9jwCBhlt9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7fdef76-048a-49d4-b9e3-592dd8ba6fdb"
      },
      "source": [
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dog\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}